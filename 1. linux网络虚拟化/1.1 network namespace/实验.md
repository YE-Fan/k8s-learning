## 实验参考地址

1.  youtube的视频   Network Namespaces Basics Explained in 15 Minutes   https://www.youtube.com/watch?v=j_UUnlVC2Ss      

2. 博客  https://tanzu.vmware.com/developer/blog/a-container-is-a-linux-namespace-and-networking-basics/

## 实验1：2个namespace联通

```sh
# 方便起见，使用root用户

# 添加namespace  red 和 blue
ip netns add red
ip netns add blue


# 添加veth pair   一端叫 veth-red  一端叫 veth-blue
ip link add veth-red type veth peer name veth-blue

# veth-red移动到netns red,  veth-blue移动到netns blue
ip link set veth-red netns red
ip link set veth-blue netns blue

# 给veth pair添加ip地址
ip -n  red  addr add 10.1.1.1/24 dev veth-red
ip -n  blue addr add 10.1.1.2/24 dev veth-blue

# 状态设置为up，创建出来默认是down
ip -n red  link set veth-red up
ip -n blue  link set veth-blue up

# 在一端ping另一端
ip netns exec red   ping 10.1.1.2
ip netns exec blue  ping 10.1.1.1

```





## 实验2：2个namespace和1个bridge组成局域网

```sh
# 方便起见，使用root用户

# 添加namespace  red 和 blue
ip netns add red
ip netns add blue

# 添加bridge   br0, 并设置为up状态
ip link add name br0 type bridge
ip link set br0 up

# 添加veth pair
ip link add veth-red type veth peer name veth-red-br0
ip link add veth-blue type veth peer name veth-blue-br0

# 一端连接到netns
ip link set veth-red netns red
ip link set veth-blue netns blue

# 一端连接到bridge
ip link set veth-red-br0 master br0
ip link set veth-blue-br0 master br0

# veth pair都设置为up
ip -n red  link set veth-red up
ip link set veth-red-br0 up
ip -n blue  link set veth-blue up
ip link set veth-blue-br0 up

# 全部添加ip地址
ip -n  red  addr add 10.1.1.1/24 dev veth-red
ip -n  blue addr add 10.1.1.2/24 dev veth-blue
ip addr add 10.1.1.3/24 dev br0

# 此时这三者应该能互相ping通
# br0 ping red和blue
ping 10.1.1.1
ping 10.1.1.2
# red ping br0
ip netns exec red ping 10.1.1.3
# blue ping br0
ip netns exec blue ping 10.1.1.3
# red和blue互相ping
ip netns exec red ping 10.1.1.2
ip netns exec blue ping 10.1.1.1
```

> red和blue可能ping不通
>
> 按照网上的说法，排查出的原因是主机装了docker，系统为bridge开启了iptables功能，导致所有经过br0的数据包都要受iptables里面规则的限制，而docker为了安全性，将iptables里面filter表的FORWARD链的默认策略设置成了drop，于是所有不符合docker规则的数据包都不会被forward，导致这种情况ping不通。
>
> 解决办法有两个，二选一：
>
> 1. 关闭系统bridge的iptables功能，这样数据包转发就不受iptables影响了：echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
> 2. 为br0添加一条iptables规则，让经过br0的包能被forward：iptables -A FORWARD -i br0 -j ACCEPT
>
> 第一种方法不确定会不会影响docker，建议用第二种方法。

> 还有一种可能是系统没有开IPv4转发，没有路由器功能，可以用下面命令开启
>
> echo 1 > /proc/sys/net/ipv4/ip_forward

> 排查方式：参考此篇https://segmentfault.com/q/1010000010011053
>
> 具体的排查过程：
>
> 在veth-red， veth-blue， br0都开启tcpdump，然后在red里ping blue
>
> 此时发现，red和br0都有ICMP包，但是blue上却没有
>
> 反过来blue ping red同理
>
> 但是br0和blue、red是都通的（前面他们互相ping都通了）
>
> 说明可能是br0未转发。然后网上提到  /proc/sys/net/ipv4/ip_forward 和iptables都可能影响这一块。
>
> cat   /proc/sys/net/ipv4/ip_forward 发现是1没错
>
> iptables -vnL & iptables -t nat -vnL 查看iptables规则，iptables暂时不太熟，直接尝试了网友的解决方法
>
> iptables -A FORWARD -i br0 -j ACCEPT
>
> 果然ping通了
>
> ---
>
> 后面重新开了台纯净的ubuntu机器，没装docker，iptables -vnL & iptables -t nat -vnL就很清爽
>
> 不用开ip_forward， 也不用修改nat，直接就能red和blue ping通



> 此处还有坑



## 实验3：在实验2的基础上，blue里访问连接host的外部机器

记得用纯净的机器，别用装了docker的，会有影响

首先我们知道，host都有一张默认网卡和外部联通，一般叫eth0，现在有些可能是其它命名规则，比如我的是ens33。

ens33是 192.168.56.129  外部机器是 192.168.56.128

现在要在实验2的基础上，blue里ping通外部机器.128



```sh
# 一开始先在blue里ping一次ens33
ip netns exec blue ping 192.168.56.129
# 不通


# 让发给192.168.56.0/24从br0走，br0成为我们的网关
ip netns exec blue ip route add 192.168.56.0/24 via 10.1.1.3

# 再在blue里ping一次ens33
ip netns exec blue ping 192.168.56.129
# 通了

# 在blue里ping一次外部机器
ip netns exec blue ping 192.168.56.128
# 不通，此时在128的机器抓不到来自10.1.1.2的ICMP包，因为我们的ubuntu机器默认没有开启包转发的功能
# 抓包命令 tcpdump -n

# 开启转发，让linux能转发包
echo 1 > /proc/sys/net/ipv4/ip_forward
# 不通，此时在128的机器能抓到包，看到 10.1.1.2 > 192.168.56.128: ICMP echo request 某某某某
# 原因是128的机器不知道10.1.1.2在哪，它只知道192.168.56.0/24的网络


# 添加nat, 让数据包从ens33出去的时候，进行一次SNAT，把数据包的源地址从10.1.1.2，改成192.168.56.128
# 因为.128的机器只知道192.168.56.0/24网络，不知道10.1.1.2在哪
iptables \
-t nat \
-A POSTROUTING \
-s 10.1.1.0/24 \
-j MASQUERADE


# 此时再在blue里ping一次外部机器
ip netns exec blue ping 192.168.56.128
# 通了
# 此时128的机器上抓到的包是
192.168.56.129 > 192.168.56.128: ICMP echo request, 某某某某
192.168.56.128 > 192.168.56.129: ICMP echo reply, 某某某某
```



## 实验4：在实验3的基础上，blue里访问公网

ping 8.8.8.8

道理和实验3是一样的，只不过我们不知道要访问公网的什么机器，所以要添加默认路由，让没有特殊指定的包，都从br0这个网关走



```sh
ip netns exec blue ping 8.8.8.8
# 不通


# 默认路由，都从br0走
ip netns exec blue ip route add default via 10.1.1.3

# 再ping
ip netns exec blue ping 8.8.8.8
# 已经通了
```





## 实验5：在实验4 的基础上，host外部访问blue里面

即.128的机器，访问.129的机器里面的blue，10.1.1.2



当然，我们可以和前面一样，在.128的机器上加上路由，让包先路由到.129的机器，129的机器那边再路由到blue里面



此处我们使用另一种方式。DNAT进行端口转发，类似于负载均衡

```sh
# 让所有访问.129的机器的80端口的，都转发到blue的80端口
# iptables -t nat -A PREROUTING --dport 80 --to-destination 10.1.1.2:80 -j DNAT
iptables -t nat -A PREROUTING -p tcp --dport 80  -j DNAT --to-destination 10.1.1.2:80
# 开启抓包
```

然后在128的机器上

```
curl 192.168.56.129
```

此时就能在129的机器上，blue里面抓到包

```sh
18:12:33.680832 IP 192.168.56.128.55034 > 10.1.1.2.http: Flags [S], seq 2377696730, win 64240, options [mss 1460,sackOK,TS val 4134010551 ecr 0,nop,wscale 7], length 0

18:12:33.680851 IP 10.1.1.2.http > 192.168.56.128.55034: Flags [R.], seq 0, ack 2377696731, win 0, length 0

# 显然，是收到了来自192.168.56.128发给10.1.1.2的包，还是http协议的，然后发了个ack包回去
```

